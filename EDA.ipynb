{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "71b96d5b",
   "metadata": {},
   "source": [
    "# По README"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "498b415a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from datasets import Dataset\n",
    "# Load the QA-form knowledge base as huggingfact datasets\n",
    "dataset_relationships = Dataset.from_dict(torch.load(\"synthetic_data/family_relationships.pt\")) #load the facts in family relationships\n",
    "dataset_biographies = Dataset.from_dict(torch.load(\"synthetic_data/family_biographies.pt\")) #load the facts in biographies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "60f249da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Relationships dataset: 400\n",
      "Biographies dataset: 300\n",
      "Sample relationships: {'fact': \"Richard Perry is Reid Perry's father.\", 'question': \"Richard Perry is Reid Perry's \", 'answer': 'father', 'question2': \"Is Richard Perry Reid Perry's father?\", 'answer2': 'yes', 'question3': \"Is Richard Perry Reid Perry's uncle?\", 'answer3': 'no', 'question4': 'Who is Richard Perry to Reid Perry?', 'answer4': 'father'}\n",
      "Sample biographies: {'fact': 'The birthyear of Sloane Lee is 1908.', 'question': 'The birthyear of Sloane Lee is', 'answer': '1908', 'question4': 'What is the birthyear of Sloane Lee?', 'answer4': '1908'}\n",
      "len_sample_relat 9\n",
      "len_sample_bio 5\n"
     ]
    }
   ],
   "source": [
    "# От авторов: The question and answer are with the key question4 and answer4. (Значит, только на них тестировалось забывание?)\n",
    "\n",
    "# Biographies забывались лучше \n",
    "print(\"Relationships dataset:\", len(dataset_relationships))\n",
    "print(\"Biographies dataset:\", len(dataset_biographies))\n",
    "print(\"Sample relationships:\", dataset_relationships[0])\n",
    "print(\"Sample biographies:\", dataset_biographies[0])\n",
    "\n",
    "sample_relat = dataset_relationships[0]\n",
    "sample_bio = dataset_biographies[0] # здесь только пары question-answer и question4-answer4\n",
    "print('len_sample_relat', len(sample_relat))\n",
    "print('len_sample_bio', len(sample_bio))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a570077",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Все длины порций данных в dataset_relationships, dataset_biographies одинаковы\n",
    "for i in range(len(dataset_relationships)):\n",
    "    if len(dataset_relationships[i]) != 9:\n",
    "        print(i)\n",
    "\n",
    "for i in range(len(dataset_biographies)):\n",
    "    if len(dataset_biographies[i]) != 5:\n",
    "        print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2eaa0b82",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Вместо 1) weights_only=False лучше делать 2) torch.serialization.add_safe_globals([Rule, Person])\n",
    "# Т.к. 1) разрешает загрузку произвольного кода, а 2) явно дополняет безопасный по умолчанию список\n",
    "# Безопасный список по умолчанию: torch.Tensor, int, float, str, list, dict, tuple, numpy arrays, built-in Python types\n",
    "\n",
    "# Load the rule set\n",
    "# from utils_data_building import Person, Rule\n",
    "rule_list = torch.load(\"synthetic_data/family_rule.pt\", weights_only=False)\n",
    "\n",
    "# Load the relationships as a list of tuple КАК ОН ПОЛУЧЕН?\n",
    "#edge_list is a list of pairs of two people; relation_list is a list of relationthips in string, e.g. child.\n",
    "(edge_list,relation_list, _, _) = torch.load(\"synthetic_data/family-200-graph.pt\", weights_only=False) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d20d997c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Число правил 48\n",
      "Type: <class 'utils_data_building.Rule'>\n",
      "Attributes: ['__class__', '__delattr__', '__dict__', '__dir__', '__doc__', '__eq__', '__format__', '__ge__', '__getattribute__', '__getstate__', '__gt__', '__hash__', '__init__', '__init_subclass__', '__le__', '__lt__', '__module__', '__ne__', '__new__', '__reduce__', '__reduce_ex__', '__repr__', '__setattr__', '__sizeof__', '__str__', '__subclasshook__', '__weakref__', 'get_dc_edges_list', 'get_up_edges_list', 'left_tuples', 'num_var', 'right_tuple']\n"
     ]
    }
   ],
   "source": [
    "print('Число правил', len(rule_list))\n",
    "first_rule = rule_list[0]\n",
    "print(f\"Type: {type(first_rule)}\")\n",
    "print(f\"Attributes: {dir(first_rule)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da36fd16",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Всего правил: 48\n",
      "\n",
      "==================================================\n",
      "\n",
      "Правило 0:\n",
      "  num_var: 2\n",
      "  left_tuples: [(0, 'wife', 1)]\n",
      "  right_tuple: (1, 'husband', 0)\n",
      "\n",
      "Правило 1:\n",
      "  num_var: 3\n",
      "  left_tuples: [(1, 'child', 2), (0, 'child', 2)]\n",
      "  right_tuple: (0, 'husband', 1)\n",
      "\n",
      "Правило 2:\n",
      "  num_var: 3\n",
      "  left_tuples: [(0, 'father', 2), (0, 'mother', 1)]\n",
      "  right_tuple: (1, 'husband', 2)\n",
      "\n",
      "Правило 3:\n",
      "  num_var: 2\n",
      "  left_tuples: [(0, 'husband', 1)]\n",
      "  right_tuple: (1, 'wife', 0)\n",
      "\n",
      "Правило 4:\n",
      "  num_var: 3\n",
      "  left_tuples: [(1, 'child', 2), (0, 'child', 2)]\n",
      "  right_tuple: (0, 'wife', 1)\n"
     ]
    }
   ],
   "source": [
    "# Проанализируем несколько правил подробнее\n",
    "print(f\"Всего правил: {len(rule_list)}\")\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "\n",
    "for i, rule in enumerate(rule_list[:5]):  # первые 3 правила\n",
    "    print(f\"\\nПравило {i}:\")\n",
    "    print(f\"  num_var: {rule.num_var}\")\n",
    "    print(f\"  left_tuples: {rule.left_tuples}\")\n",
    "    print(f\"  right_tuple: {rule.right_tuple}\")\n",
    "    \n",
    "    # # Вызовем методы чтобы увидеть что они возвращают\n",
    "    # if hasattr(rule, 'get_dc_edges_list'):\n",
    "    #     print(f\"  get_dc_edges_list(): {rule.get_dc_edges_list()}\")\n",
    "    # if hasattr(rule, 'get_up_edges_list'):\n",
    "    #     print(f\"  get_up_edges_list(): {rule.get_up_edges_list()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c54ec404",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils_data_building import Person, Rule\n",
    "# Load the relationships as a list of tuple\n",
    "#edge_list is a list of pairs of two people; relation_list is a list of relationthips in string, e.g. child.\n",
    "(edge_list,relation_list, _, _) = torch.load(\"synthetic_data/family-200-graph.pt\", weights_only=False) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b964fe0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "400\n",
      "400\n",
      "(69, 67)\n",
      "father\n",
      "Type: <class 'tuple'>\n",
      "Attributes: ['__add__', '__class__', '__class_getitem__', '__contains__', '__delattr__', '__dir__', '__doc__', '__eq__', '__format__', '__ge__', '__getattribute__', '__getitem__', '__getnewargs__', '__getstate__', '__gt__', '__hash__', '__init__', '__init_subclass__', '__iter__', '__le__', '__len__', '__lt__', '__mul__', '__ne__', '__new__', '__reduce__', '__reduce_ex__', '__repr__', '__rmul__', '__setattr__', '__sizeof__', '__str__', '__subclasshook__', 'count', 'index']\n"
     ]
    }
   ],
   "source": [
    "print(len(edge_list))\n",
    "print(len(relation_list))\n",
    "print(edge_list[0])\n",
    "print(relation_list[0])\n",
    "print(f\"Type: {type(edge_list[0])}\")\n",
    "print(f\"Attributes: {dir(edge_list[0])}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bab66d8",
   "metadata": {},
   "source": [
    "# Разбор кода"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb7add07",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Код из calculate_recall_and_acc.py\n",
    "(edge_list, edge_type_list, fixed_names, person_list) = torch.load(\"synthetic_data/family-200-graph.pt\", weights_only=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20dc90f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "400\n",
      "400\n",
      "100\n",
      "100\n",
      "(69, 67)\n",
      "father\n",
      "Sloane Lee\n",
      "<utils_data_building.Person object at 0x7e13e265ea80>\n",
      "\n",
      "Type: <class 'utils_data_building.Person'>\n",
      "Attributes: ['__class__', '__delattr__', '__dict__', '__dir__', '__doc__', '__eq__', '__format__', '__ge__', '__getattribute__', '__getstate__', '__gt__', '__hash__', '__init__', '__init_subclass__', '__le__', '__lt__', '__module__', '__ne__', '__new__', '__reduce__', '__reduce_ex__', '__repr__', '__setattr__', '__sizeof__', '__str__', '__subclasshook__', '__weakref__', 'age', 'birthplace', 'children', 'father', 'gender', 'generation', 'husband', 'if_build', 'job', 'mother', 'name', 'wife']\n"
     ]
    }
   ],
   "source": [
    "print(len(edge_list))\n",
    "print(len(edge_type_list))\n",
    "print(len(fixed_names))\n",
    "print(len(person_list))\n",
    "print(edge_list[0])\n",
    "print(edge_type_list[0])\n",
    "print(fixed_names[0])\n",
    "print(person_list[0])\n",
    "print()\n",
    "print(f\"Type: {type(person_list[0])}\")\n",
    "print(f\"Attributes: {dir(person_list[0])}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d9eea80",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Всего людей: 100\n",
      "\n",
      "==================================================\n",
      "Человек 0:\n",
      "  age: 1908\n",
      "  birthplace: [46]\n",
      "  children: [<utils_data_building.Person object at 0x7e13e15143b0>, <utils_data_building.Person object at 0x7e13e152c800>]\n",
      "  father: None\n",
      "  gender: female\n",
      "  generation: 0\n",
      "  husband: <utils_data_building.Person object at 0x7e13e15150a0>\n",
      "  if_build: False\n",
      "  job: Banker\n",
      "  mother: None\n",
      "  name: 0\n",
      "  wife: None\n"
     ]
    }
   ],
   "source": [
    "# Проанализируем несколько Person подробнее\n",
    "print(f\"Всего людей: {len(person_list)}\")\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "\n",
    "for i, person in enumerate(person_list[:1]):  # первые 3 правила\n",
    "    print(f\"Человек {i}:\")\n",
    "    print(f\"  age: {person.age}\")\n",
    "    print(f\"  birthplace: {person.birthplace}\")\n",
    "    print(f\"  children: {person.children}\")\n",
    "    print(f\"  father: {person.father}\")\n",
    "    print(f\"  gender: {person.gender}\")\n",
    "    print(f\"  generation: {person.generation}\")\n",
    "    print(f\"  husband: {person.husband}\")\n",
    "    print(f\"  if_build: {person.if_build}\")\n",
    "    print(f\"  job: {person.job}\")\n",
    "    print(f\"  mother: {person.mother}\")\n",
    "    print(f\"  name: {person.name}\")\n",
    "    print(f\"  wife: {person.wife}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "089c97d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "55\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([267, 272,  45, 108,  79,  97,  32, 220, 145, 300,  61, 374, 311,\n",
       "       280,  29, 290,  68, 146, 122,  77,  60,  51, 116, 274, 127, 343,\n",
       "       324, 260, 344, 286, 275, 229,  78, 219, 375, 349, 283, 282, 244,\n",
       "       186, 304,  28,  11, 106,  96, 180, 224, 171, 250, 269, 392, 265,\n",
       "       214,  59, 187])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# We select a random subset of size 55 from the facts in family relationship to evaluate the deep unlearning.\n",
    "shuffled_edge_id_list = torch.load(\"synthetic_data/subsample.pt\", weights_only=False)\n",
    "print(len(shuffled_edge_id_list))\n",
    "shuffled_edge_id_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d770516",
   "metadata": {},
   "outputs": [],
   "source": [
    "shuffled_unlearn_data_id = shuffled_edge_id_list[args.unlearn_data_id]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e6f1992",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.int64(187)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#  Given any unlearn_target_data_id in 0-54, the id of fact in relationships is\n",
    "# shuffled_unlearn_data_id = shuffled_edge_id_list[unlearn_target_data_id]\n",
    "shuffled_unlearn_data_id = shuffled_edge_id_list[54]\n",
    "shuffled_unlearn_data_id"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f365b6d0",
   "metadata": {},
   "source": [
    "# Проверка скрипта calculate_acc_and_recall"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfa11cf7",
   "metadata": {},
   "source": [
    "Надо понять разницу между relationships, biographies: biographies_correct.pt vs family_biographies.pt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6a9e884",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Надо понять, как метрики ниже вычислены. Это усреднение всех методов? Надо ли что-то учить"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc2c878f",
   "metadata": {},
   "source": [
    "For any unlearning method, suppose relationships_correct.pt and biographies_correct.pt are two 0-1 vectors saved under the directory input_dir, which indicate the retained facts in family relationships and biographies after the unlearning the fact unlearn_target_data_id."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b2338f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# При запуске (для unlearn_data_id 2 результаты те же)\n",
    "\n",
    "python calculate_recall_and_acc.py --unlearn_data_id 1 --input_dir example_for_evaluation\n",
    "# но по смыслу расчета для accuracy of biographies надо 1 - 0.023 = 0.977 \n",
    "('recall', 'accuracy of relationships', 'accuracy of biographies', 'accuracy of all knowledge base')\n",
    "(0.75, 0.25, 0.023333333333333334, 0.15229885057471265)\n",
    "\n",
    "# python calculate_recall_and_acc.py --unlearn_data_id 3 --input_dir example_for_evaluation\n",
    "# (1.0, 0.2518891687657431, 0.023333333333333334, 0.15351506456241032)\n",
    "\n",
    "# Что это значит? # Видимо пообъектно рассчитываются метрики.\n",
    "# upd: скорее качество модели оценивается после подачи ей одного факта\n",
    "\n",
    "# Вот только я запуталась, это superficial или deep unlearning? По идее deep, но тут смесь с фактами из биографии\n",
    "# Как их отучать? Подать сразу все 300 фактов?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b348df5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Тип данных: <class 'list'>\n",
      "Размер/форма: 400\n",
      "Содержимое: [False, True, True, False, False, True, False, False, True, True, False, False, True, True, False, False, False, False, False, False, True, False, False, False, False, False, False, False, False, False, True, False, False, False, False, True, False, True, False, True, False, True, True, False, False, False, False, False, False, True, False, False, False, False, True, True, False, False, True, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, True, True, False, False, True, False, False, False, False, False, False, True, False, True, False, True, False, False, False, False, False, False, False, True, False, False, True, False, False, False, False, False, False, False, True, False, False, False, False, False, False, True, True, False, True, True, False, False, False, True, False, False, True, True, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, True, False, False, False, False, False, True, True, False, True, True, False, False, False, False, False, False, False, False, False, False, False, False, False, True, False, False, True, False, False, False, True, False, True, False, True, True, False, False, False, False, False, False, False, False, False, False, False, True, True, False, False, False, False, False, True, False, False, False, True, False, False, False, True, False, False, False, False, False, True, True, False, True, True, False, False, False, False, False, False, True, True, False, False, True, False, False, False, False, True, False, False, False, False, False, True, False, False, False, False, True, True, True, False, False, False, True, False, False, False, False, False, False, False, False, True, False, False, True, False, False, False, False, False, False, False, True, True, True, True, False, False, False, False, True, False, False, False, False, True, False, False, False, True, True, False, False, False, False, False, True, False, False, False, False, False, False, False, False, True, False, False, True, False, False, False, True, False, False, False, True, False, True, True, False, False, False, False, False, True, False, False, False, True, True, True, True, False, False, True, False, False, False, True, True, False, False, False, True, False, False, False, False, True, True, False, False, True, True, False, False, False, False, True, False, False, False, False, True, False, False, False, False, False, False, False, False, True, False, False, True, True, True, False, False, False, True, False, False, False, False, False]\n"
     ]
    }
   ],
   "source": [
    "# Загрузка файла (бинарный 0/1 False/True)\n",
    "data = torch.load(\"example_for_evaluation/relationships_correct.pt\")  # если связывать это с vllm_eval.py, то True = факт сохранился после unlearning\n",
    "\n",
    "# Просмотр базовой информации\n",
    "print(\"Тип данных:\", type(data))\n",
    "print(\"Размер/форма:\", data.shape if hasattr(data, 'shape') else len(data))\n",
    "print(\"Содержимое:\", data)\n",
    "\n",
    "# Если это тензор\n",
    "if torch.is_tensor(data):\n",
    "    print(\"Форма тензора:\", data.shape)\n",
    "    print(\"Тип данных тензора:\", data.dtype)\n",
    "    print(\"Первые 10 элементов:\", data[:10] if len(data) > 10 else data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "022bd817",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Тип данных: <class 'list'>\n",
      "Размер/форма: 300\n",
      "Содержимое: [False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, True, True, False, False, True, False, False, True, False, False, False, False, False, False, False, False, False, False, False, False, False, True, False, False, False, False, False, False, False, False, False, False, False, False, True, False, False, False, False, False, False, False, False, False, False, False, True, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False]\n",
      "Число единиц: 7\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "data = torch.load(\"example_for_evaluation/biographies_correct.pt\")\n",
    "\n",
    "print(\"Тип данных:\", type(data))\n",
    "print(\"Размер/форма:\", data.shape if hasattr(data, 'shape') else len(data))\n",
    "print(\"Содержимое:\", data)\n",
    "print('Число единиц:', np.asarray(data).sum())\n",
    "\n",
    "# Если это тензор\n",
    "if torch.is_tensor(data):\n",
    "    print(\"Форма тензора:\", data.shape)\n",
    "    print(\"Тип данных тензора:\", data.dtype)\n",
    "    print(\"Первые 10 элементов:\", data[:10] if len(data) > 10 else data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebc9cf6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "rel_ind = np.asarray(torch.load(f\"{args.input_dir}/relationships_correct.pt\")).astype(np.float32)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c94abe9e",
   "metadata": {},
   "source": [
    "# Повторение забывания и forget.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcb104ce",
   "metadata": {},
   "source": [
    "In the paper, we tested with four unlearning methods: gradient ascent (GA), Negative preference optimization  (NPO), task vector (TV), who's harry potter (WHP). The hyperparameter list of each method is saved in config/model_config.yaml. The related scripts are saved in ./unlearning_methods. By set any unlearning_method (ga, npo, tv, whp), any target_model (phi, gpt2-xl, llama2-7b, llama3-8b), and unlearn_target_data_id (0-54), the script is"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0f26349",
   "metadata": {},
   "outputs": [],
   "source": [
    "bash scripts_unlearning_methods/${unlearning_methods}.sh $target_model $unlearn_target_data_id\n",
    "\n",
    "# например\n",
    "bash unlearning_methods/ga.sh gpt2-xl 1\n",
    "\n",
    "В ga.sh будет forget.py, vllm_eval.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd5fb623",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Просмотреть forget.py и vllm_eval.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ece785bf",
   "metadata": {},
   "source": [
    "1. В forget.py нет отдельной логики для tv и whp, есть лишь для npo и ga. Возможно, разные технические реализации\n",
    "2. unlearn_trainer.py - ядро забывания, в нем же описаны лишь ga и npo\n",
    "3. В forget.py в hydra нужно монтировать google drive, чтобы грузить отттуда/туда чекпоинты\n",
    "4. В data_module.py создаются батчи из многократно повторенного 1 и того же факта, вроде эффективно для обучения\n",
    "\n",
    "5. Изучить ядро забывания - unlearn_trainer.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5f2c14c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 01.10.2025. Продолжить с get_minimal_nec_unlearn_and_not_included_unlearn в utils_metric.py\n",
    "# 05.10.2025. Что там такое minimal_set?\n",
    "# Понять, как считается accuracy. Что такое, minimal_set на 80% стало понятно"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d55324e8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "6397d904",
   "metadata": {},
   "source": [
    "06.10.2025. \n",
    "1. Как получаются векторы из 0/1 relationships_correct.pt и biographies_correct.pt после unlearning? Допустим, выбирается 1 из 4 моделей и 1 из 4 методов. Но каких данных происходит unlearning? \n",
    "2. Здесь очень много о том, как создать правильное множество для забывания факта. Но не ясно, на каких данных происходит забывание.\n",
    "3. По идее сколько фактов, столько раз и происходил unlearning * 4 * 4 (моделей * методов)\n",
    "Какие именно данные подаются в модели при unlearning?\n",
    "4. По какой все же логике строятся ответы при запуске python calculate_recall_and_acc.py --unlearn_data_id 3 --input_dir example_for_evaluation ? Или "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "251d13cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'tuple'>\n",
      "Размер/форма: 4\n",
      "Содержимое: (np.float64(1.0), np.float64(0.2518891687657431), np.float64(0.023333333333333334), np.float64(0.15351506456241032))\n"
     ]
    }
   ],
   "source": [
    "# В rec_acc данные только по проверке 1 id. И это тот, который я запускала в последний раз.\n",
    "# Значит, от авторов ничего содержательного не сохранилось\n",
    "rec_acc = torch.load('example_for_evaluation/rec_acc.pt', weights_only=False)\n",
    "print(type(rec_acc))\n",
    "print(\"Размер/форма:\", rec_acc.shape if hasattr(data, 'shape') else len(rec_acc))\n",
    "print(\"Содержимое:\", rec_acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aedd25ae",
   "metadata": {},
   "source": [
    "!!! Понять, как создаются relationships_correct.pt  (rel_ind, unlearn_ind на его основе) и biographies_correct.pt (bio_ind), т.к. на их основе считаются потом метрики, это существенно важно для оценки"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68dda071",
   "metadata": {},
   "source": [
    "07.10.2025\n",
    "1.Изучить ga.sh, forget.py, vllm_eval.py, utils_metric.py, unlearn_trainer.py\n",
    "2.Чтобы понять, как на взятом одном примере провести unlearning, куда он будет сохраняться. Сколько времени уходит на 1 образец. Нужно для него разные чекпоинты посохранять. Желательно и каждую эпоху, и в лог. шкале (1, 2, 4, 16, 32)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28e2aac6",
   "metadata": {},
   "source": [
    "# 12.10.2025"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abb4a067",
   "metadata": {},
   "source": [
    "Удалось запустить bash scripts_unlearning_methods/ga.sh gpt2_xl 1 и bash scripts_unlearning_methods/ga.sh gpt2_xl 2.\n",
    "\n",
    "Есть проблемы:\n",
    "1. Не ясно до конца, что именно сохранилось (например, что тако biographies_correct.pt) и что не сохранилось из того, что должно. Провести аналогию с уже понятым примером расчета accuracy, recall\n",
    "2. Не ясно, что с логгированием (создаются пустые папки)\n",
    "3. Есть проблемы AttributeError: module 'lm_eval.utils' has no attribute 'eval_logger'\n",
    "4. При этом есть ощущение, что google drive сильно забился и непонятно, чем, будто что-то сохраняется, но я не понимаю, куда.\n",
    "5. Не понятен порядок выведения метрик в терминале. По номеру чекпоинта в лог шкале?\n",
    "\n",
    "Понять, как точно считаются метрики в выводах в консоль. И причем там biographies (поможет calculate). Вообще не ясно, как по нему считается."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6806bf1",
   "metadata": {},
   "source": [
    "# 14.10.2025"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c53b051",
   "metadata": {},
   "outputs": [],
   "source": [
    "При запуске 2 основные проблемы\n",
    "1. Сохраняются данные не всех чекпоинтов, причем не сохраняются всегда для разных моделей. \n",
    "И я заметила, что веса моделей в корзине есть для всех чекпоинтов => проблема именно в запуске оценки и генерации _correct, _responses файлов \n",
    "(ПРОВЕРИТЬ ОБЯЗАТЕЛЬНО)\n",
    "По идее, либо модель не загружается, либо модель не записывается для оценки\n",
    "INFO 10-14 20:11:16 model_runner.py:879] Starting to load model /content/drive/MyDrive/Unlearning/models/unlearning_checkpoint/ga/gpt2_xl/3/checkpoint-32/...\n",
    "\n",
    "\n",
    "\n",
    "2. Логи не сохраняются. В простыне терминала смотреть метрики вообще не удобно\n",
    "3. В 96 строке lm_eval/models/whp_huggingface.py есть eval_logger = utils.eval_logger, но в utils нет метода eval_logger\n",
    "Добавила в lm_eval/models/utils.py `eval_logger = logging.getLogger(\"lm_eval\")``\n",
    "4. lm-eval не работает (а это оценка качества на дополнительных датасетах)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "170a6e7c",
   "metadata": {},
   "source": [
    "gpt2 unlearn_id 2 (порядок оценки чекпоинтов произволен, это особ-ть файл. с-мы)\n",
    "checkpoint-16: relationships accuracy: 0.1825, biographies accuracy: 0.8033\n",
    "checkpoint-2:  relationships accuracy: 0.9975, biographies accuracy: 1.0\n",
    "checkpoint-4:  relationships accuracy: 0.9475, biographies accuracy: 1.0  \n",
    "checkpoint-8:  relationships accuracy: 0.4950, biographies accuracy: 0.9933"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c223d07f",
   "metadata": {},
   "source": [
    "Если я правильно пон, accuracy на biographies здесь не должна проседать вовсе, это как характеристика остаточных знаний модели"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c8265bb",
   "metadata": {},
   "source": [
    "# 15.10.2025"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dedfbfad",
   "metadata": {},
   "source": [
    "1. Т.к. чекпоинты сохраняются через раз, проверить теорию с сохранением, пока не получится, с пом. colab. Возможно, дело в нестабильности соединения и тд\n",
    "2. Далее исследовать метрики из логов и файлы _responses, _correct\n",
    "3. Подумать, нужно ли создание логов или все будет рассчитываться как в vllm_eval.py (там считается accuracy) и так"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f501dffa",
   "metadata": {},
   "source": [
    "# 19.10.2025"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4593cec2",
   "metadata": {},
   "source": [
    "1. Дописан метод reliable_save_tensors в unlearn_trainer.py => теперь сохраняются все чекпоинты => vllm_eval.py корректно сохраняет результаты, можно считать accuracy\n",
    "2. Возникла проблема распараллеливания и модель пыталась сохраниться сразу обоими gpu=> \"stage3_gather_16bit_weights_on_model_save\": true изменено на false (вроде ок, т.к. у меня все равно 1 gpu)  (ПРОТЕСТИРОВАТЬ)\n",
    "3. Все еще не сохраняются логи (и не ясно, какие должны). Это станет проблемой, когда я начну что-то запускать, скажем, на ночь\n",
    "В логи нужно:\n",
    "-метрики для каждой эпохи в виде\n",
    "{'loss': -84.0, 'grad_norm': 97.88301348381403, 'learning_rate': 1.2903225806451614e-06, 'epoch': 30.0} \n",
    "-accuracy для чекпоинтов в виде\n",
    "Dataset({\n",
    "    features: ['fact', 'question', 'answer', 'question2', 'answer2', 'question3', 'answer3', 'question4', 'answer4'],\n",
    "    num_rows: 400\n",
    "})accuracy: 0.3449999988079071\n",
    "Processed prompts: 100%|██████████████████████████████████████████████████████████████████████| 300/300 [00:00<00:00, 915.71it/s, est. speed input: 12385.94 toks/s, output: 3831.41 toks/s]\n",
    "Dataset({\n",
    "    features: ['fact', 'question', 'answer', 'question4', 'answer4'],\n",
    "    num_rows: 300\n",
    "})accuracy: 0.9566666483879089\n",
    "-факт успешности сохранения чекпоинта\n",
    "4. lm_eval надо бы починить (предположительно, взять более старый коммит), чтобы видеть качество модели на общих данных\n",
    "5. В этил логах нет расчета recall. Как из этого выводится recall ?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e4fa208",
   "metadata": {},
   "source": [
    "**ВСПОМНИТЬ, КАК ПОЛУЧАЛСЯ RECALL, Т.К. В ЭТИХ ЛОГАХ ЕГО НЕТ**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c19cb36",
   "metadata": {},
   "source": [
    "# vllm_eval.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "301522b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_dataset_list = [Dataset.from_dict(torch.load(\"synthetic_data/family_relationships.pt\", weights_only=False)), Dataset.from_dict(torch.load(\"synthetic_data/family_biographies.pt\", weights_only=False))]\n",
    "eval_dataset_name_list = [\"relationships_\", \"biographies_\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "baa7a9c3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['fact', 'question', 'answer', 'question2', 'answer2', 'question3', 'answer3', 'question4', 'answer4'],\n",
       "    num_rows: 400\n",
       "})"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Dataset.from_dict(torch.load(\"synthetic_data/family_relationships.pt\", weights_only=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e33d1b5",
   "metadata": {},
   "source": [
    "В каждом чекпоинте есть (например по этому пути: ls /content/drive/MyDrive/Unlearning/models/unlearning_checkpoint/ga/gpt2_xl/2/checkpoint-32)\n",
    "\n",
    "biographies_correct.pt  \n",
    "biographies_responses.pt  \n",
    "merges.txt  \n",
    "relationships_correct.pt  \n",
    "relationships_responses.pt\n",
    "\n",
    "Проверим содержимое"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "84595707",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "300\n",
      "[True, True, True]\n"
     ]
    }
   ],
   "source": [
    "bio_correct = 'biographies_correct.pt'\n",
    "path = f\"/content/drive/MyDrive/Unlearning/models/unlearning_checkpoint/ga/gpt2_xl/2/checkpoint-32/{bio_correct}\"\n",
    "\n",
    "# загружаем\n",
    "data_bio_correct = torch.load(path)\n",
    "\n",
    "# смотрим, что внутри\n",
    "print(len(data_bio_correct))\n",
    "print(data_bio_correct[:3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3909c4ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install -q vllm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2c3672cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "300\n",
      "Элемент 0:\n",
      "Тип: <class 'vllm.outputs.RequestOutput'>\n",
      "Атрибуты: ['request_id', 'prompt', 'prompt_token_ids', 'prompt_logprobs', 'outputs', 'finished', 'metrics', 'lora_request', 'encoder_prompt', 'encoder_prompt_token_ids']\n",
      "Элемент 1:\n",
      "Тип: <class 'vllm.outputs.RequestOutput'>\n",
      "Атрибуты: ['request_id', 'prompt', 'prompt_token_ids', 'prompt_logprobs', 'outputs', 'finished', 'metrics', 'lora_request', 'encoder_prompt', 'encoder_prompt_token_ids']\n",
      "\n",
      "=== Элемент 0 ===\n",
      "request_id: 400\n",
      "prompt (первые 100 символов): Question: What is the birthyear of Sloane Lee?\n",
      " Answer:\n",
      "outputs: [CompletionOutput(index=0, text='1908fatherfatherfatherfatherfatherfatherfatherfather', token_ids=array('l', [1129, 2919, 11358, 11358, 11358, 11358, 11358, 11358, 11358, 11358]), cumulative_logprob=None, logprobs=None, finish_reason=length, stop_reason=None)]\n",
      "finished: True\n",
      "metrics: RequestMetrics(arrival_time=1760881789.375499, last_token_time=1760881789.375499, first_scheduled_time=1760881789.4262805, first_token_time=1760881789.4734387, time_in_queue=0.0507814884185791, finished_time=1760881789.9282935, scheduler_time=0.0219563860046037, model_forward_time=None, model_execute_time=None)\n",
      "\n",
      "=== Элемент 1 ===\n",
      "request_id: 401\n",
      "prompt (первые 100 символов): Question: What is the birthplace of Sloane Lee?\n",
      " Answer:\n",
      "outputs: [CompletionOutput(index=0, text='Washington statefatherfatherfatherfatherfatherfatherfatherfather', token_ids=array('l', [17402, 1181, 11358, 11358, 11358, 11358, 11358, 11358, 11358, 11358]), cumulative_logprob=None, logprobs=None, finish_reason=length, stop_reason=None)]\n",
      "finished: True\n",
      "metrics: RequestMetrics(arrival_time=1760881789.3759916, last_token_time=1760881789.3759916, first_scheduled_time=1760881789.4262805, first_token_time=1760881789.4734387, time_in_queue=0.05028891563415527, finished_time=1760881789.9283018, scheduler_time=0.0219563860046037, model_forward_time=None, model_execute_time=None)\n",
      "\n",
      "=== Элемент 2 ===\n",
      "request_id: 402\n",
      "prompt (первые 100 символов): Question: What is the job of Sloane Lee?\n",
      " Answer:\n",
      "outputs: [CompletionOutput(index=0, text='Bankerfatherfatherfatherfatherfatherfatherfatherfather', token_ids=array('l', [28650, 263, 11358, 11358, 11358, 11358, 11358, 11358, 11358, 11358]), cumulative_logprob=None, logprobs=None, finish_reason=length, stop_reason=None)]\n",
      "finished: True\n",
      "metrics: RequestMetrics(arrival_time=1760881789.3762283, last_token_time=1760881789.3762283, first_scheduled_time=1760881789.4262805, first_token_time=1760881789.4734387, time_in_queue=0.05005216598510742, finished_time=1760881789.9283059, scheduler_time=0.0219563860046037, model_forward_time=None, model_execute_time=None)\n",
      "\n",
      "=== Элемент 3 ===\n",
      "request_id: 403\n",
      "prompt (первые 100 символов): Question: What is the birthyear of Zane Ross?\n",
      " Answer:\n",
      "outputs: [CompletionOutput(index=0, text='1901fatherfatherfatherfatherfatherfatherfatherfather', token_ids=array('l', [1129, 486, 11358, 11358, 11358, 11358, 11358, 11358, 11358, 11358]), cumulative_logprob=None, logprobs=None, finish_reason=length, stop_reason=None)]\n",
      "finished: True\n",
      "metrics: RequestMetrics(arrival_time=1760881789.3764122, last_token_time=1760881789.3764122, first_scheduled_time=1760881789.4262805, first_token_time=1760881789.4734387, time_in_queue=0.04986834526062012, finished_time=1760881789.9283092, scheduler_time=0.0219563860046037, model_forward_time=None, model_execute_time=None)\n",
      "\n",
      "=== Элемент 4 ===\n",
      "request_id: 404\n",
      "prompt (первые 100 символов): Question: What is the birthplace of Zane Ross?\n",
      " Answer:\n",
      "outputs: [CompletionOutput(index=0, text='Ohio statefatherfatherfatherfatherfatherfatherfatherfather', token_ids=array('l', [31274, 1181, 11358, 11358, 11358, 11358, 11358, 11358, 11358, 11358]), cumulative_logprob=None, logprobs=None, finish_reason=length, stop_reason=None)]\n",
      "finished: True\n",
      "metrics: RequestMetrics(arrival_time=1760881789.3765757, last_token_time=1760881789.3765757, first_scheduled_time=1760881789.4262805, first_token_time=1760881789.4734387, time_in_queue=0.049704790115356445, finished_time=1760881789.9283123, scheduler_time=0.0219563860046037, model_forward_time=None, model_execute_time=None)\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import torch\n",
    "sys.path.append(\"/content/deep_unlearning\")  # путь, где лежит evaluate_util.py\n",
    "import evaluate_util\n",
    "\n",
    "bio_responses = 'biographies_responses.pt'\n",
    "path = f\"/content/drive/MyDrive/Unlearning/models/unlearning_checkpoint/ga/gpt2_xl/2/checkpoint-32/{bio_responses}\"\n",
    "\n",
    "# загружаем\n",
    "data_bio_responses = torch.load(path, weights_only=False) \n",
    "\n",
    "# смотрим, что внутри\n",
    "print(len(data_bio_responses))\n",
    "# простой print тут падает с ошибкой\n",
    "# безопасно посмотрим первые два объекта\n",
    "for i, obj in enumerate(data_bio_responses[:2]):\n",
    "    print(f\"Элемент {i}:\")\n",
    "    print(\"Тип:\", type(obj))\n",
    "    # если объект словарь или имеет атрибуты __dict__\n",
    "    if hasattr(obj, \"__dict__\"):\n",
    "        print(\"Атрибуты:\", list(obj.__dict__.keys()))\n",
    "\n",
    "for i, obj in enumerate(data_bio_responses[:5]):\n",
    "    print(f\"\\n=== Элемент {i} ===\")\n",
    "    print(\"request_id:\", getattr(obj, \"request_id\", None))\n",
    "    print(\"prompt (первые 100 символов):\", getattr(obj, \"prompt\", \"\")[:100])\n",
    "    print(\"outputs:\", getattr(obj, \"outputs\", None))\n",
    "    print(\"finished:\", getattr(obj, \"finished\", None))\n",
    "    print(\"metrics:\", getattr(obj, \"metrics\", None))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c83981b7",
   "metadata": {},
   "source": [
    "**Важные выводы**\n",
    "\n",
    "Особенно важная логика в vllm_eval.py (здесь формируется biographies_correct.pt, relationships_correct.pt = булевы массивы результатов unlearning, где True = факт сохранен)и в calculate_recall_and_acc.py (здесь корректная логика расчета acc_rel, acc_bio, rec_rel)\n",
    "\n",
    "Отдельно стоит отметить, что acc_rel, выводимая в логах unlearning и рассчитываемая в vllm_eval.py считается как среднее числа единиц по массиву размера 400, что считается корректным для biographies, но не считается корректным для relationships"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e837832",
   "metadata": {},
   "source": [
    "В цикле протестировать, как получить acc, recall для разных relationships_correct.pt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "eac46810",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on class LLM in module vllm.entrypoints.llm:\n",
      "\n",
      "class LLM(builtins.object)\n",
      " |  LLM(model: str, *, runner: Literal['auto', 'generate', 'pooling', 'draft'] = 'auto', convert: Literal['auto', 'none', 'embed', 'classify', 'reward'] = 'auto', tokenizer: Optional[str] = None, tokenizer_mode: Literal['auto', 'slow', 'mistral', 'custom'] = 'auto', skip_tokenizer_init: bool = False, trust_remote_code: bool = False, allowed_local_media_path: str = '', allowed_media_domains: Optional[list[str]] = None, tensor_parallel_size: int = 1, dtype: Literal['auto', 'half', 'float16', 'bfloat16', 'float', 'float32'] = 'auto', quantization: Optional[Literal['awq', 'deepspeedfp', 'tpu_int8', 'fp8', 'ptpc_fp8', 'fbgemm_fp8', 'modelopt', 'modelopt_fp4', 'bitblas', 'gguf', 'gptq_marlin_24', 'gptq_marlin', 'gptq_bitblas', 'awq_marlin', 'gptq', 'compressed-tensors', 'bitsandbytes', 'hqq', 'experts_int8', 'ipex', 'quark', 'moe_wna16', 'torchao', 'auto-round', 'rtn', 'inc', 'mxfp4', 'petit_nvfp4']] = None, revision: Optional[str] = None, tokenizer_revision: Optional[str] = None, seed: Optional[int] = None, gpu_memory_utilization: float = 0.9, swap_space: float = 4, cpu_offload_gb: float = 0, enforce_eager: bool = False, disable_custom_all_reduce: bool = False, hf_token: Union[bool, str, NoneType] = None, hf_overrides: Union[dict[str, Any], Callable[[Any], Any], NoneType] = None, mm_processor_kwargs: Optional[dict[str, Any]] = None, pooler_config: Optional[vllm.config.pooler.PoolerConfig] = None, override_pooler_config: Optional[vllm.config.pooler.PoolerConfig] = None, structured_outputs_config: Union[dict[str, Any], vllm.config.structured_outputs.StructuredOutputsConfig, NoneType] = None, kv_cache_memory_bytes: Optional[int] = None, compilation_config: Union[int, dict[str, Any], vllm.config.compilation.CompilationConfig, NoneType] = None, logits_processors: Optional[list[Union[str, type[vllm.v1.sample.logits_processor.interface.LogitsProcessor]]]] = None, **kwargs: Any) -> None\n",
      " |\n",
      " |  An LLM for generating texts from given prompts and sampling parameters.\n",
      " |\n",
      " |  This class includes a tokenizer, a language model (possibly distributed\n",
      " |  across multiple GPUs), and GPU memory space allocated for intermediate\n",
      " |  states (aka KV cache). Given a batch of prompts and sampling parameters,\n",
      " |  this class generates texts from the model, using an intelligent batching\n",
      " |  mechanism and efficient memory management.\n",
      " |\n",
      " |  Args:\n",
      " |      model: The name or path of a HuggingFace Transformers model.\n",
      " |      tokenizer: The name or path of a HuggingFace Transformers tokenizer.\n",
      " |      tokenizer_mode: The tokenizer mode. \"auto\" will use the fast tokenizer\n",
      " |          if available, and \"slow\" will always use the slow tokenizer.\n",
      " |      skip_tokenizer_init: If true, skip initialization of tokenizer and\n",
      " |          detokenizer. Expect valid prompt_token_ids and None for prompt\n",
      " |          from the input.\n",
      " |      trust_remote_code: Trust remote code (e.g., from HuggingFace) when\n",
      " |          downloading the model and tokenizer.\n",
      " |      allowed_local_media_path: Allowing API requests to read local images\n",
      " |          or videos from directories specified by the server file system.\n",
      " |          This is a security risk. Should only be enabled in trusted\n",
      " |          environments.\n",
      " |      allowed_media_domains: If set, only media URLs that belong to this\n",
      " |          domain can be used for multi-modal inputs.\n",
      " |      tensor_parallel_size: The number of GPUs to use for distributed\n",
      " |          execution with tensor parallelism.\n",
      " |      dtype: The data type for the model weights and activations. Currently,\n",
      " |          we support `float32`, `float16`, and `bfloat16`. If `auto`, we use\n",
      " |          the `torch_dtype` attribute specified in the model config file.\n",
      " |          However, if the `torch_dtype` in the config is `float32`, we will\n",
      " |          use `float16` instead.\n",
      " |      quantization: The method used to quantize the model weights. Currently,\n",
      " |          we support \"awq\", \"gptq\", and \"fp8\" (experimental).\n",
      " |          If None, we first check the `quantization_config` attribute in the\n",
      " |          model config file. If that is None, we assume the model weights are\n",
      " |          not quantized and use `dtype` to determine the data type of\n",
      " |          the weights.\n",
      " |      revision: The specific model version to use. It can be a branch name,\n",
      " |          a tag name, or a commit id.\n",
      " |      tokenizer_revision: The specific tokenizer version to use. It can be a\n",
      " |          branch name, a tag name, or a commit id.\n",
      " |      seed: The seed to initialize the random number generator for sampling.\n",
      " |      gpu_memory_utilization: The ratio (between 0 and 1) of GPU memory to\n",
      " |          reserve for the model weights, activations, and KV cache. Higher\n",
      " |          values will increase the KV cache size and thus improve the model's\n",
      " |          throughput. However, if the value is too high, it may cause out-of-\n",
      " |          memory (OOM) errors.\n",
      " |      kv_cache_memory_bytes: Size of KV Cache per GPU in bytes. By default,\n",
      " |          this is set to None and vllm can automatically infer the kv cache\n",
      " |          size based on gpu_memory_utilization. However, users may want to\n",
      " |          manually specify the kv cache memory size. kv_cache_memory_bytes\n",
      " |          allows more fine-grain control of how much memory gets used when\n",
      " |          compared with using gpu_memory_memory_utilization. Note that\n",
      " |          kv_cache_memory_bytes (when not-None) ignores\n",
      " |          gpu_memory_utilization\n",
      " |      swap_space: The size (GiB) of CPU memory per GPU to use as swap space.\n",
      " |          This can be used for temporarily storing the states of the requests\n",
      " |          when their `best_of` sampling parameters are larger than 1. If all\n",
      " |          requests will have `best_of=1`, you can safely set this to 0.\n",
      " |          Noting that `best_of` is only supported in V0. Otherwise, too small\n",
      " |          values may cause out-of-memory (OOM) errors.\n",
      " |      cpu_offload_gb: The size (GiB) of CPU memory to use for offloading\n",
      " |          the model weights. This virtually increases the GPU memory space\n",
      " |          you can use to hold the model weights, at the cost of CPU-GPU data\n",
      " |          transfer for every forward pass.\n",
      " |      enforce_eager: Whether to enforce eager execution. If True, we will\n",
      " |          disable CUDA graph and always execute the model in eager mode.\n",
      " |          If False, we will use CUDA graph and eager execution in hybrid.\n",
      " |      disable_custom_all_reduce: See\n",
      " |          [ParallelConfig][vllm.config.ParallelConfig].\n",
      " |      hf_token: The token to use as HTTP bearer authorization for remote files\n",
      " |          . If `True`, will use the token generated when running\n",
      " |          `huggingface-cli login` (stored in `~/.huggingface`).\n",
      " |      hf_overrides: If a dictionary, contains arguments to be forwarded to the\n",
      " |          HuggingFace config. If a callable, it is called to update the\n",
      " |          HuggingFace config.\n",
      " |      mm_processor_kwargs: Arguments to be forwarded to the model's processor\n",
      " |          for multi-modal data, e.g., image processor. Overrides for the\n",
      " |          multi-modal processor obtained from `AutoProcessor.from_pretrained`.\n",
      " |          The available overrides depend on the model that is being run.\n",
      " |          For example, for Phi-3-Vision: `{\"num_crops\": 4}`.\n",
      " |      pooler_config: Initialize non-default pooling config for the pooling\n",
      " |          model. e.g. `PoolerConfig(pooling_type=\"mean\", normalize=False)`.\n",
      " |      override_pooler_config: [DEPRECATED] Use `pooler_config` instead. This\n",
      " |          argument is deprecated and will be removed in v0.12.0 or v1.0.0,\n",
      " |          whichever is sooner.\n",
      " |      compilation_config: Either an integer or a dictionary. If it is an\n",
      " |          integer, it is used as the level of compilation optimization. If it\n",
      " |          is a dictionary, it can specify the full compilation configuration.\n",
      " |      **kwargs: Arguments for [`EngineArgs`][vllm.EngineArgs].\n",
      " |\n",
      " |  Note:\n",
      " |      This class is intended to be used for offline inference. For online\n",
      " |      serving, use the [AsyncLLMEngine][vllm.AsyncLLMEngine] class instead.\n",
      " |\n",
      " |  Methods defined here:\n",
      " |\n",
      " |  __init__(self, model: str, *, runner: Literal['auto', 'generate', 'pooling', 'draft'] = 'auto', convert: Literal['auto', 'none', 'embed', 'classify', 'reward'] = 'auto', tokenizer: Optional[str] = None, tokenizer_mode: Literal['auto', 'slow', 'mistral', 'custom'] = 'auto', skip_tokenizer_init: bool = False, trust_remote_code: bool = False, allowed_local_media_path: str = '', allowed_media_domains: Optional[list[str]] = None, tensor_parallel_size: int = 1, dtype: Literal['auto', 'half', 'float16', 'bfloat16', 'float', 'float32'] = 'auto', quantization: Optional[Literal['awq', 'deepspeedfp', 'tpu_int8', 'fp8', 'ptpc_fp8', 'fbgemm_fp8', 'modelopt', 'modelopt_fp4', 'bitblas', 'gguf', 'gptq_marlin_24', 'gptq_marlin', 'gptq_bitblas', 'awq_marlin', 'gptq', 'compressed-tensors', 'bitsandbytes', 'hqq', 'experts_int8', 'ipex', 'quark', 'moe_wna16', 'torchao', 'auto-round', 'rtn', 'inc', 'mxfp4', 'petit_nvfp4']] = None, revision: Optional[str] = None, tokenizer_revision: Optional[str] = None, seed: Optional[int] = None, gpu_memory_utilization: float = 0.9, swap_space: float = 4, cpu_offload_gb: float = 0, enforce_eager: bool = False, disable_custom_all_reduce: bool = False, hf_token: Union[bool, str, NoneType] = None, hf_overrides: Union[dict[str, Any], Callable[[Any], Any], NoneType] = None, mm_processor_kwargs: Optional[dict[str, Any]] = None, pooler_config: Optional[vllm.config.pooler.PoolerConfig] = None, override_pooler_config: Optional[vllm.config.pooler.PoolerConfig] = None, structured_outputs_config: Union[dict[str, Any], vllm.config.structured_outputs.StructuredOutputsConfig, NoneType] = None, kv_cache_memory_bytes: Optional[int] = None, compilation_config: Union[int, dict[str, Any], vllm.config.compilation.CompilationConfig, NoneType] = None, logits_processors: Optional[list[Union[str, type[vllm.v1.sample.logits_processor.interface.LogitsProcessor]]]] = None, **kwargs: Any) -> None\n",
      " |      LLM constructor.\n",
      " |\n",
      " |  apply_model(self, func: Callable[[torch.nn.modules.module.Module], ~_R]) -> list[~_R]\n",
      " |      Run a function directly on the model inside each worker,\n",
      " |      returning the result for each of them.\n",
      " |\n",
      " |      !!! warning\n",
      " |          To reduce the overhead of data transfer, avoid returning large\n",
      " |          arrays or tensors from this method. If you must return them,\n",
      " |          make sure you move them to CPU first to avoid taking up additional\n",
      " |          VRAM!\n",
      " |\n",
      " |  beam_search(self, prompts: list[typing.Union[vllm.inputs.data.TokensPrompt, vllm.inputs.data.TextPrompt]], params: vllm.sampling_params.BeamSearchParams, lora_request: Union[list[vllm.lora.request.LoRARequest], vllm.lora.request.LoRARequest, NoneType] = None, use_tqdm: bool = False, concurrency_limit: Optional[int] = None) -> list[vllm.beam_search.BeamSearchOutput]\n",
      " |      Generate sequences using beam search.\n",
      " |\n",
      " |      Args:\n",
      " |          prompts: A list of prompts. Each prompt can be a string or a list\n",
      " |              of token IDs.\n",
      " |          params: The beam search parameters.\n",
      " |          lora_request: LoRA request to use for generation, if any.\n",
      " |          use_tqdm: Whether to use tqdm to display the progress bar.\n",
      " |          concurrency_limit: The maximum number of concurrent requests.\n",
      " |              If None, the number of concurrent requests is unlimited.\n",
      " |\n",
      " |  chat(self, messages: Union[list[Union[openai.types.chat.chat_completion_developer_message_param.ChatCompletionDeveloperMessageParam, openai.types.chat.chat_completion_system_message_param.ChatCompletionSystemMessageParam, openai.types.chat.chat_completion_user_message_param.ChatCompletionUserMessageParam, openai.types.chat.chat_completion_assistant_message_param.ChatCompletionAssistantMessageParam, openai.types.chat.chat_completion_tool_message_param.ChatCompletionToolMessageParam, openai.types.chat.chat_completion_function_message_param.ChatCompletionFunctionMessageParam, vllm.entrypoints.chat_utils.CustomChatCompletionMessageParam, openai_harmony.Message]], list[list[Union[openai.types.chat.chat_completion_developer_message_param.ChatCompletionDeveloperMessageParam, openai.types.chat.chat_completion_system_message_param.ChatCompletionSystemMessageParam, openai.types.chat.chat_completion_user_message_param.ChatCompletionUserMessageParam, openai.types.chat.chat_completion_assistant_message_param.ChatCompletionAssistantMessageParam, openai.types.chat.chat_completion_tool_message_param.ChatCompletionToolMessageParam, openai.types.chat.chat_completion_function_message_param.ChatCompletionFunctionMessageParam, vllm.entrypoints.chat_utils.CustomChatCompletionMessageParam, openai_harmony.Message]]]], sampling_params: Union[vllm.sampling_params.SamplingParams, list[vllm.sampling_params.SamplingParams], NoneType] = None, use_tqdm: Union[bool, Callable[..., tqdm.auto.tqdm]] = True, lora_request: Optional[vllm.lora.request.LoRARequest] = None, chat_template: Optional[str] = None, chat_template_content_format: Literal['auto', 'string', 'openai'] = 'auto', add_generation_prompt: bool = True, continue_final_message: bool = False, tools: Optional[list[dict[str, Any]]] = None, chat_template_kwargs: Optional[dict[str, Any]] = None, mm_processor_kwargs: Optional[dict[str, Any]] = None) -> list[vllm.outputs.RequestOutput]\n",
      " |      Generate responses for a chat conversation.\n",
      " |\n",
      " |      The chat conversation is converted into a text prompt using the\n",
      " |      tokenizer and calls the [generate][vllm.LLM.generate] method to generate\n",
      " |      the responses.\n",
      " |\n",
      " |      Multi-modal inputs can be passed in the same way you would pass them\n",
      " |      to the OpenAI API.\n",
      " |\n",
      " |      Args:\n",
      " |          messages: A list of conversations or a single conversation.\n",
      " |\n",
      " |              - Each conversation is represented as a list of messages.\n",
      " |              - Each message is a dictionary with 'role' and 'content' keys.\n",
      " |\n",
      " |          sampling_params: The sampling parameters for text generation.\n",
      " |              If None, we use the default sampling parameters. When it\n",
      " |              is a single value, it is applied to every prompt. When it\n",
      " |              is a list, the list must have the same length as the\n",
      " |              prompts and it is paired one by one with the prompt.\n",
      " |          use_tqdm: If `True`, shows a tqdm progress bar.\n",
      " |              If a callable (e.g., `functools.partial(tqdm, leave=False)`),\n",
      " |              it is used to create the progress bar.\n",
      " |              If `False`, no progress bar is created.\n",
      " |          lora_request: LoRA request to use for generation, if any.\n",
      " |          chat_template: The template to use for structuring the chat.\n",
      " |              If not provided, the model's default chat template will be used.\n",
      " |          chat_template_content_format: The format to render message content.\n",
      " |\n",
      " |              - \"string\" will render the content as a string.\n",
      " |                Example: `\"Who are you?\"`\n",
      " |              - \"openai\" will render the content as a list of dictionaries,\n",
      " |                similar to OpenAI schema.\n",
      " |                Example: `[{\"type\": \"text\", \"text\": \"Who are you?\"}]`\n",
      " |\n",
      " |          add_generation_prompt: If True, adds a generation template\n",
      " |              to each message.\n",
      " |          continue_final_message: If True, continues the final message in\n",
      " |              the conversation instead of starting a new one. Cannot be\n",
      " |              `True` if `add_generation_prompt` is also `True`.\n",
      " |          chat_template_kwargs: Additional kwargs to pass to the chat\n",
      " |              template.\n",
      " |          mm_processor_kwargs: Multimodal processor kwarg overrides for this\n",
      " |              chat request. Only used for offline requests.\n",
      " |\n",
      " |      Returns:\n",
      " |          A list of `RequestOutput` objects containing the generated\n",
      " |          responses in the same order as the input messages.\n",
      " |\n",
      " |  classify(self, prompts: Union[str, vllm.inputs.data.TextPrompt, vllm.inputs.data.TokensPrompt, vllm.inputs.data.EmbedsPrompt, vllm.inputs.data.ExplicitEncoderDecoderPrompt, collections.abc.Sequence[Union[str, vllm.inputs.data.TextPrompt, vllm.inputs.data.TokensPrompt, vllm.inputs.data.EmbedsPrompt, vllm.inputs.data.ExplicitEncoderDecoderPrompt]]], *, use_tqdm: Union[bool, Callable[..., tqdm.auto.tqdm]] = True, pooling_params: Union[vllm.pooling_params.PoolingParams, collections.abc.Sequence[vllm.pooling_params.PoolingParams], NoneType] = None, lora_request: Union[list[vllm.lora.request.LoRARequest], vllm.lora.request.LoRARequest, NoneType] = None) -> list[vllm.outputs.ClassificationRequestOutput]\n",
      " |      Generate class logits for each prompt.\n",
      " |\n",
      " |      This class automatically batches the given prompts, considering\n",
      " |      the memory constraint. For the best performance, put all of your prompts\n",
      " |      into a single list and pass it to this method.\n",
      " |\n",
      " |      Args:\n",
      " |          prompts: The prompts to the LLM. You may pass a sequence of prompts\n",
      " |              for batch inference. See [PromptType][vllm.inputs.PromptType]\n",
      " |              for more details about the format of each prompt.\n",
      " |          use_tqdm: If `True`, shows a tqdm progress bar.\n",
      " |              If a callable (e.g., `functools.partial(tqdm, leave=False)`),\n",
      " |              it is used to create the progress bar.\n",
      " |              If `False`, no progress bar is created.\n",
      " |          lora_request: LoRA request to use for generation, if any.\n",
      " |          pooling_params: The pooling parameters for pooling. If None, we\n",
      " |              use the default pooling parameters.\n",
      " |      Returns:\n",
      " |          A list of `ClassificationRequestOutput` objects containing the\n",
      " |          embedding vectors in the same order as the input prompts.\n",
      " |\n",
      " |  collective_rpc(self, method: Union[str, Callable[..., ~_R]], timeout: Optional[float] = None, args: tuple = (), kwargs: Optional[dict[str, Any]] = None) -> list[~_R]\n",
      " |      Execute an RPC call on all workers.\n",
      " |\n",
      " |      Args:\n",
      " |          method: Name of the worker method to execute, or a callable that\n",
      " |              is serialized and sent to all workers to execute.\n",
      " |\n",
      " |              If the method is a callable, it should accept an additional\n",
      " |              `self` argument, in addition to the arguments passed in `args`\n",
      " |              and `kwargs`. The `self` argument will be the worker object.\n",
      " |          timeout: Maximum time in seconds to wait for execution. Raises a\n",
      " |              [`TimeoutError`][] on timeout. `None` means wait indefinitely.\n",
      " |          args: Positional arguments to pass to the worker method.\n",
      " |          kwargs: Keyword arguments to pass to the worker method.\n",
      " |\n",
      " |      Returns:\n",
      " |          A list containing the results from each worker.\n",
      " |\n",
      " |      Note:\n",
      " |          It is recommended to use this API to only pass control messages,\n",
      " |          and set up data-plane communication to pass data.\n",
      " |\n",
      " |  embed(self, prompts: Union[str, vllm.inputs.data.TextPrompt, vllm.inputs.data.TokensPrompt, vllm.inputs.data.EmbedsPrompt, vllm.inputs.data.ExplicitEncoderDecoderPrompt, collections.abc.Sequence[Union[str, vllm.inputs.data.TextPrompt, vllm.inputs.data.TokensPrompt, vllm.inputs.data.EmbedsPrompt, vllm.inputs.data.ExplicitEncoderDecoderPrompt]]], *, truncate_prompt_tokens: Optional[int] = None, use_tqdm: Union[bool, Callable[..., tqdm.auto.tqdm]] = True, pooling_params: Union[vllm.pooling_params.PoolingParams, collections.abc.Sequence[vllm.pooling_params.PoolingParams], NoneType] = None, lora_request: Union[list[vllm.lora.request.LoRARequest], vllm.lora.request.LoRARequest, NoneType] = None) -> list[vllm.outputs.EmbeddingRequestOutput]\n",
      " |      Generate an embedding vector for each prompt.\n",
      " |\n",
      " |      This class automatically batches the given prompts, considering\n",
      " |      the memory constraint. For the best performance, put all of your prompts\n",
      " |      into a single list and pass it to this method.\n",
      " |\n",
      " |      Args:\n",
      " |          prompts: The prompts to the LLM. You may pass a sequence of prompts\n",
      " |              for batch inference. See [PromptType][vllm.inputs.PromptType]\n",
      " |              for more details about the format of each prompt.\n",
      " |          pooling_params: The pooling parameters for pooling. If None, we\n",
      " |              use the default pooling parameters.\n",
      " |          use_tqdm: If `True`, shows a tqdm progress bar.\n",
      " |              If a callable (e.g., `functools.partial(tqdm, leave=False)`),\n",
      " |              it is used to create the progress bar.\n",
      " |              If `False`, no progress bar is created.\n",
      " |          lora_request: LoRA request to use for generation, if any.\n",
      " |\n",
      " |      Returns:\n",
      " |          A list of `EmbeddingRequestOutput` objects containing the\n",
      " |          embedding vectors in the same order as the input prompts.\n",
      " |\n",
      " |  encode(self, prompts: Union[str, vllm.inputs.data.TextPrompt, vllm.inputs.data.TokensPrompt, vllm.inputs.data.EmbedsPrompt, vllm.inputs.data.ExplicitEncoderDecoderPrompt, collections.abc.Sequence[Union[str, vllm.inputs.data.TextPrompt, vllm.inputs.data.TokensPrompt, vllm.inputs.data.EmbedsPrompt, vllm.inputs.data.ExplicitEncoderDecoderPrompt]], vllm.inputs.data.DataPrompt], pooling_params: Union[vllm.pooling_params.PoolingParams, collections.abc.Sequence[vllm.pooling_params.PoolingParams], NoneType] = None, *, truncate_prompt_tokens: Optional[int] = None, use_tqdm: Union[bool, Callable[..., tqdm.auto.tqdm]] = True, lora_request: Union[list[vllm.lora.request.LoRARequest], vllm.lora.request.LoRARequest, NoneType] = None, pooling_task: Literal['encode', 'embed', 'classify', 'score'] = 'encode', tokenization_kwargs: Optional[dict[str, Any]] = None) -> list[vllm.outputs.PoolingRequestOutput]\n",
      " |      Apply pooling to the hidden states corresponding to the input\n",
      " |      prompts.\n",
      " |\n",
      " |      This class automatically batches the given prompts, considering\n",
      " |      the memory constraint. For the best performance, put all of your prompts\n",
      " |      into a single list and pass it to this method.\n",
      " |\n",
      " |      Args:\n",
      " |          prompts: The prompts to the LLM. You may pass a sequence of prompts\n",
      " |              for batch inference. See [PromptType][vllm.inputs.PromptType]\n",
      " |              for more details about the format of each prompt.\n",
      " |          pooling_params: The pooling parameters for pooling. If None, we\n",
      " |              use the default pooling parameters.\n",
      " |          use_tqdm: If `True`, shows a tqdm progress bar.\n",
      " |              If a callable (e.g., `functools.partial(tqdm, leave=False)`),\n",
      " |              it is used to create the progress bar.\n",
      " |              If `False`, no progress bar is created.\n",
      " |          lora_request: LoRA request to use for generation, if any.\n",
      " |          pooling_task: Override the pooling task to use.\n",
      " |          tokenization_kwargs: overrides tokenization_kwargs set in\n",
      " |              pooling_params\n",
      " |\n",
      " |      Returns:\n",
      " |          A list of `PoolingRequestOutput` objects containing the\n",
      " |          pooled hidden states in the same order as the input prompts.\n",
      " |\n",
      " |      Note:\n",
      " |          Using `prompts` and `prompt_token_ids` as keyword parameters is\n",
      " |          considered legacy and may be deprecated in the future. You should\n",
      " |          instead pass them via the `inputs` parameter.\n",
      " |\n",
      " |  generate(self, prompts: Union[str, vllm.inputs.data.TextPrompt, vllm.inputs.data.TokensPrompt, vllm.inputs.data.EmbedsPrompt, vllm.inputs.data.ExplicitEncoderDecoderPrompt, collections.abc.Sequence[Union[str, vllm.inputs.data.TextPrompt, vllm.inputs.data.TokensPrompt, vllm.inputs.data.EmbedsPrompt, vllm.inputs.data.ExplicitEncoderDecoderPrompt]]], sampling_params: Union[vllm.sampling_params.SamplingParams, collections.abc.Sequence[vllm.sampling_params.SamplingParams], NoneType] = None, *, use_tqdm: Union[bool, Callable[..., tqdm.auto.tqdm]] = True, lora_request: Union[list[vllm.lora.request.LoRARequest], vllm.lora.request.LoRARequest, NoneType] = None, priority: Optional[list[int]] = None) -> list[vllm.outputs.RequestOutput]\n",
      " |      Generates the completions for the input prompts.\n",
      " |\n",
      " |      This class automatically batches the given prompts, considering\n",
      " |      the memory constraint. For the best performance, put all of your prompts\n",
      " |      into a single list and pass it to this method.\n",
      " |\n",
      " |      Args:\n",
      " |          prompts: The prompts to the LLM. You may pass a sequence of prompts\n",
      " |              for batch inference. See [PromptType][vllm.inputs.PromptType]\n",
      " |              for more details about the format of each prompt.\n",
      " |          sampling_params: The sampling parameters for text generation. If\n",
      " |              None, we use the default sampling parameters.\n",
      " |              When it is a single value, it is applied to every prompt.\n",
      " |              When it is a list, the list must have the same length as the\n",
      " |              prompts and it is paired one by one with the prompt.\n",
      " |          use_tqdm: If `True`, shows a tqdm progress bar.\n",
      " |              If a callable (e.g., `functools.partial(tqdm, leave=False)`),\n",
      " |              it is used to create the progress bar.\n",
      " |              If `False`, no progress bar is created.\n",
      " |          lora_request: LoRA request to use for generation, if any.\n",
      " |          priority: The priority of the requests, if any.\n",
      " |              Only applicable when priority scheduling policy is enabled.\n",
      " |\n",
      " |      Returns:\n",
      " |          A list of `RequestOutput` objects containing the\n",
      " |          generated completions in the same order as the input prompts.\n",
      " |\n",
      " |      Note:\n",
      " |          Using `prompts` and `prompt_token_ids` as keyword parameters is\n",
      " |          considered legacy and may be deprecated in the future. You should\n",
      " |          instead pass them via the `inputs` parameter.\n",
      " |\n",
      " |  get_default_sampling_params(self) -> vllm.sampling_params.SamplingParams\n",
      " |\n",
      " |  get_metrics(self) -> list['Metric']\n",
      " |      Return a snapshot of aggregated metrics from Prometheus.\n",
      " |\n",
      " |      Returns:\n",
      " |          A ``MetricSnapshot`` instance capturing the current state\n",
      " |          of all aggregated metrics from Prometheus.\n",
      " |\n",
      " |      Note:\n",
      " |          This method is only available with the V1 LLM engine.\n",
      " |\n",
      " |  get_tokenizer(self) -> Union[transformers.tokenization_utils.PreTrainedTokenizer, transformers.tokenization_utils_fast.PreTrainedTokenizerFast, Any]\n",
      " |\n",
      " |  preprocess_chat(self, messages: Union[list[Union[openai.types.chat.chat_completion_developer_message_param.ChatCompletionDeveloperMessageParam, openai.types.chat.chat_completion_system_message_param.ChatCompletionSystemMessageParam, openai.types.chat.chat_completion_user_message_param.ChatCompletionUserMessageParam, openai.types.chat.chat_completion_assistant_message_param.ChatCompletionAssistantMessageParam, openai.types.chat.chat_completion_tool_message_param.ChatCompletionToolMessageParam, openai.types.chat.chat_completion_function_message_param.ChatCompletionFunctionMessageParam, vllm.entrypoints.chat_utils.CustomChatCompletionMessageParam, openai_harmony.Message]], list[list[Union[openai.types.chat.chat_completion_developer_message_param.ChatCompletionDeveloperMessageParam, openai.types.chat.chat_completion_system_message_param.ChatCompletionSystemMessageParam, openai.types.chat.chat_completion_user_message_param.ChatCompletionUserMessageParam, openai.types.chat.chat_completion_assistant_message_param.ChatCompletionAssistantMessageParam, openai.types.chat.chat_completion_tool_message_param.ChatCompletionToolMessageParam, openai.types.chat.chat_completion_function_message_param.ChatCompletionFunctionMessageParam, vllm.entrypoints.chat_utils.CustomChatCompletionMessageParam, openai_harmony.Message]]]], chat_template: Optional[str] = None, chat_template_content_format: Literal['auto', 'string', 'openai'] = 'auto', add_generation_prompt: bool = True, continue_final_message: bool = False, tools: Optional[list[dict[str, Any]]] = None, chat_template_kwargs: Optional[dict[str, Any]] = None, mm_processor_kwargs: Optional[dict[str, Any]] = None) -> list[vllm.inputs.data.TokensPrompt]\n",
      " |      Generate prompt for a chat conversation. The pre-processed\n",
      " |      prompt can then be used as input for the other LLM methods.\n",
      " |\n",
      " |      Refer to `chat` for a complete description of the arguments.\n",
      " |      Returns:\n",
      " |          A list of `TokensPrompts` objects containing the tokenized\n",
      " |          prompt after chat template interpolation, and the\n",
      " |          pre-processed multi-modal inputs.\n",
      " |\n",
      " |  reset_prefix_cache(self, device: Optional[vllm.utils.Device] = None) -> bool\n",
      " |\n",
      " |  reward(self, prompts: Union[str, vllm.inputs.data.TextPrompt, vllm.inputs.data.TokensPrompt, vllm.inputs.data.EmbedsPrompt, vllm.inputs.data.ExplicitEncoderDecoderPrompt, collections.abc.Sequence[Union[str, vllm.inputs.data.TextPrompt, vllm.inputs.data.TokensPrompt, vllm.inputs.data.EmbedsPrompt, vllm.inputs.data.ExplicitEncoderDecoderPrompt]]], /, *, truncate_prompt_tokens: Optional[int] = None, use_tqdm: Union[bool, Callable[..., tqdm.auto.tqdm]] = True, pooling_params: Union[vllm.pooling_params.PoolingParams, collections.abc.Sequence[vllm.pooling_params.PoolingParams], NoneType] = None, lora_request: Union[list[vllm.lora.request.LoRARequest], vllm.lora.request.LoRARequest, NoneType] = None) -> list[vllm.outputs.PoolingRequestOutput]\n",
      " |      Generate rewards for each prompt.\n",
      " |\n",
      " |      Args:\n",
      " |          prompts: The prompts to the LLM. You may pass a sequence of prompts\n",
      " |              for batch inference. See [PromptType][vllm.inputs.PromptType]\n",
      " |              for more details about the format of each prompt.\n",
      " |          use_tqdm: If `True`, shows a tqdm progress bar.\n",
      " |              If a callable (e.g., `functools.partial(tqdm, leave=False)`),\n",
      " |              it is used to create the progress bar.\n",
      " |              If `False`, no progress bar is created.\n",
      " |          lora_request: LoRA request to use for generation, if any.\n",
      " |          pooling_params: The pooling parameters for pooling. If None, we\n",
      " |              use the default pooling parameters.\n",
      " |      Returns:\n",
      " |          A list of `PoolingRequestOutput` objects containing the\n",
      " |          pooled hidden states in the same order as the input prompts.\n",
      " |\n",
      " |  score(self, data_1: Union[str, vllm.inputs.data.TextPrompt, vllm.inputs.data.TokensPrompt, vllm.inputs.data.EmbedsPrompt, collections.abc.Sequence[Union[str, vllm.inputs.data.TextPrompt, vllm.inputs.data.TokensPrompt, vllm.inputs.data.EmbedsPrompt]], vllm.entrypoints.score_utils.ScoreMultiModalParam], data_2: Union[str, vllm.inputs.data.TextPrompt, vllm.inputs.data.TokensPrompt, vllm.inputs.data.EmbedsPrompt, collections.abc.Sequence[Union[str, vllm.inputs.data.TextPrompt, vllm.inputs.data.TokensPrompt, vllm.inputs.data.EmbedsPrompt]], vllm.entrypoints.score_utils.ScoreMultiModalParam], /, *, truncate_prompt_tokens: Optional[int] = None, use_tqdm: Union[bool, Callable[..., tqdm.auto.tqdm]] = True, pooling_params: Optional[vllm.pooling_params.PoolingParams] = None, lora_request: Union[list[vllm.lora.request.LoRARequest], vllm.lora.request.LoRARequest, NoneType] = None) -> list[vllm.outputs.ScoringRequestOutput]\n",
      " |      Generate similarity scores for all pairs `<text,text_pair>` or\n",
      " |        `<multi-modal data, multi-modal data pair>`.\n",
      " |\n",
      " |      The inputs can be `1 -> 1`, `1 -> N` or `N -> N`.\n",
      " |      In the `1 - N` case the `data_1` input will be replicated `N`\n",
      " |      times to pair with the `data_2` inputs.\n",
      " |      The input pairs are used to build a list of prompts for the\n",
      " |      cross encoder model. This class automatically batches the prompts,\n",
      " |      considering the memory constraint. For the best performance, put all\n",
      " |      of your inputs into a single list and pass it to this method.\n",
      " |\n",
      " |      Supports both text and multi-modal data (images, etc.) when used with\n",
      " |      appropriate multi-modal models. For multi-modal inputs, ensure the\n",
      " |      prompt structure matches the model's expected input format.\n",
      " |\n",
      " |      Args:\n",
      " |          data_1: Can be a single prompt, a list of prompts or\n",
      " |              `ScoreMultiModalParam`, which can contain either text or\n",
      " |              multi-modal data. When a list, it must have the same length as\n",
      " |              the `data_2` list.\n",
      " |          data_2: The data to pair with the query to form the input to\n",
      " |              the LLM. Can be text or multi-modal data. See [PromptType]\n",
      " |              [vllm.inputs.PromptType] for more details about the format of\n",
      " |              each prompt.\n",
      " |          use_tqdm: If `True`, shows a tqdm progress bar.\n",
      " |              If a callable (e.g., `functools.partial(tqdm, leave=False)`),\n",
      " |              it is used to create the progress bar.\n",
      " |              If `False`, no progress bar is created.\n",
      " |          lora_request: LoRA request to use for generation, if any.\n",
      " |          pooling_params: The pooling parameters for pooling. If None, we\n",
      " |              use the default pooling parameters.\n",
      " |      Returns:\n",
      " |          A list of `ScoringRequestOutput` objects containing the\n",
      " |          generated scores in the same order as the input prompts.\n",
      " |\n",
      " |  set_tokenizer(self, tokenizer: Union[transformers.tokenization_utils.PreTrainedTokenizer, transformers.tokenization_utils_fast.PreTrainedTokenizerFast, Any]) -> None\n",
      " |\n",
      " |  sleep(self, level: int = 1)\n",
      " |      Put the engine to sleep. The engine should not process any requests.\n",
      " |      The caller should guarantee that no requests are being processed\n",
      " |      during the sleep period, before `wake_up` is called.\n",
      " |\n",
      " |      Args:\n",
      " |          level: The sleep level. Level 1 sleep will offload the model\n",
      " |              weights and discard the kv cache. The content of kv cache\n",
      " |              is forgotten. Level 1 sleep is good for sleeping and waking\n",
      " |              up the engine to run the same model again. The model weights\n",
      " |              are backed up in CPU memory. Please make sure there's enough\n",
      " |              CPU memory to store the model weights. Level 2 sleep will\n",
      " |              discard both the model weights and the kv cache. The content\n",
      " |              of both the model weights and kv cache is forgotten. Level 2\n",
      " |              sleep is good for sleeping and waking up the engine to run a\n",
      " |              different model or update the model, where previous model\n",
      " |              weights are not needed. It reduces CPU memory pressure.\n",
      " |\n",
      " |  start_profile(self) -> None\n",
      " |\n",
      " |  stop_profile(self) -> None\n",
      " |\n",
      " |  wake_up(self, tags: Optional[list[str]] = None)\n",
      " |      Wake up the engine from sleep mode. See the [sleep][vllm.LLM.sleep]\n",
      " |      method for more details.\n",
      " |\n",
      " |      Args:\n",
      " |          tags: An optional list of tags to reallocate the engine memory\n",
      " |              for specific memory allocations. Values must be in\n",
      " |              `(\"weights\", \"kv_cache\")`. If None, all memory is reallocated.\n",
      " |              wake_up should be called with all tags (or None) before the\n",
      " |              engine is used again.\n",
      " |\n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors defined here:\n",
      " |\n",
      " |  __dict__\n",
      " |      dictionary for instance variables\n",
      " |\n",
      " |  __weakref__\n",
      " |      list of weak references to the object\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from vllm import LLM\n",
    "help(LLM)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42226804",
   "metadata": {},
   "source": [
    "# 20.10.2025"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "065c7bf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Изменить код так, чтобы из корзины google drive файлы удалялись сразу"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfbcae9b",
   "metadata": {},
   "source": [
    "# 22.10.2025"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "946b0d5b",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (ipython-input-1620811485.py, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"/tmp/ipython-input-1620811485.py\"\u001b[0;36m, line \u001b[0;32m1\u001b[0m\n\u001b[0;31m    calculate_recall_and_acc.py --unlearn_data_id 1 --input_dir example_for_evaluation\u001b[0m\n\u001b[0m                                                  ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "calculate_recall_and_acc.py --unlearn_data_id 1 --input_dir example_for_evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b5914b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Надо написать скрипт, который бы брал из google drive "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
